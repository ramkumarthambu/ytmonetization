# %%
!pip install pandas numpy streamlit matplotlib seaborn scikit-learn plotly 

# %%
import pandas as pd
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# %%
# Set up matplotlib for better visualization
sns.set_style("whitegrid")


# %%
df = pd.read_csv("youtube_ad_revenue_dataset.csv")

# %%
df.head

# %%
df.info()

# %%
df.isnull().sum()

# %%
df.duplicated().sum()

# %%
#Handles missing values, removes duplicates, and prepares data for modeling.
# Handle missing values
    # Fill numerical missing values with the mean

numerical_cols = ['views', 'likes', 'comments', 'watch_time_minutes', 'video_length_minutes', 'subscribers', 'ad_revenue_usd']
for col in numerical_cols:
    if df[col].isnull().any():
        df[col] = df[col].fillna(df[col].mean())
        print(f"Filled missing values in '{col}' with the mean.")


# %%
# Fill categorical missing values with the mode
categorical_cols = ['category', 'device', 'country']
for col in categorical_cols:
    if df[col].isnull().any():
         mode_val = df[col].mode()[0]
         df[col] = df[col].fillna(mode_val)
         print(f"Filled missing values in '{col}' with the mode.")

# %%
# Remove duplicates
initial_rows = len(df)
df.drop_duplicates(inplace=True)
print(f"Removed {initial_rows - len(df)} duplicate rows.")


# %%
df.duplicated().sum()

# %%
# Create 'engagement_rate' to avoid division by zero
df['engagement_rate'] = np.where(df['views'] > 0, (df['likes'] + df['comments']) / df['views'], 0)
print("Created 'engagement_rate' feature.")


# %%

    
    #Builds and evaluates 5 different regression models.
    
print("\n--- Model Building and Evaluation ---")

    # Define features and target
features = ['views', 'likes', 'comments', 'watch_time_minutes', 'video_length_minutes', 'subscribers', 'category', 'device', 'country', 'engagement_rate']
target = 'ad_revenue_usd'

X = df[features]
y = df[target]

 # Identify numerical and categorical columns for preprocessing
numerical_features = ['views', 'likes', 'comments', 'watch_time_minutes', 'video_length_minutes', 'subscribers', 'engagement_rate']
categorical_features = ['category', 'device', 'country']

    # Create preprocessing pipelines
preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
        ])

    # Define the models
models = {
        'Linear Regression': LinearRegression(),
        'Ridge': Ridge(),
        'Lasso': Lasso(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }

    # Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train and evaluate each model
results = {}
for name, model in models.items():
        print(f"\nTraining {name}...")
        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])
        pipeline.fit(X_train, y_train)
        y_pred = pipeline.predict(X_test)

        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)

        results[name] = {'R2 Score': r2, 'MAE': mae, 'MSE': mse}
        print(f"{name} Results:")
        print(f"  R-squared: {r2:.4f}")
        print(f"  Mean Absolute Error (MAE): {mae:.2f}")
        print(f"  Mean Squared Error (MSE): {mse:.2f}")

    # Determine the best model based on R-squared
best_model_name = max(results, key=lambda name: results[name]['R2 Score'])
best_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', models[best_model_name])])
best_pipeline.fit(X, y) # Retrain on the full dataset

print(f"\n--- Best Model: {best_model_name} ---")
print(f"R2 Score: {results[best_model_name]['R2 Score']:.4f}")

    # Save the best model and preprocessing pipeline
joblib.dump(best_pipeline, 'ad_revenue_model.pkl')
print("Best model saved as 'ad_revenue_model.pkl'")

    


